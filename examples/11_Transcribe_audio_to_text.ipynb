{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhampikian/Autobiographic/blob/main/examples/11_Transcribe_audio_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pjmz-RORV8E"
      },
      "source": [
        "# Transcribe audio to text\n",
        "\n",
        "This notebook covers the transcription of audio files to text using models provided by Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk31rbYjSTYm"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "Install `txtai` and all dependencies. Since this notebook is using optional pipelines, we need to install the pipeline extras package. We'll also demonstrate running this pipeline through the API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMQuuun2R06J"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/neuml/txtai#egg=txtai[api,pipeline]\n",
        "\n",
        "# Get test data\n",
        "!wget -N https://github.com/neuml/txtai/releases/download/v3.5.0/tests.tar.gz\n",
        "!tar -xvzf tests.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNPJ95cdTKSS"
      },
      "source": [
        "# Create a Transcription instance\n",
        "\n",
        "The Transcription instance is the main entrypoint for transcribing audio to text. The pipeline abstracts transcribing audio into a one line call!\n",
        "\n",
        "The pipeline executes logic to read audio files into memory, run the data through a machine learning model and output the results to text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTDwXOUeTH2-"
      },
      "source": [
        "%%capture\n",
        "\n",
        "from txtai.pipeline import Transcription\n",
        "\n",
        "# Create transcription model\n",
        "transcribe = Transcription()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vGR_piwZZO6"
      },
      "source": [
        "# Transcribe audio to text\n",
        "\n",
        "The example below shows how to transcribe a list of audio files to text. Let's transcribe audio to text and look at each result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K2YJJzsVtfq"
      },
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "files = [\"Beijing_mobilises.wav\", \"Canadas_last_fully.wav\", \"Maine_man_wins_1_mil.wav\", \"Make_huge_profits.wav\", \"The_National_Park.wav\", \"US_tops_5_million.wav\"]\n",
        "files = [\"txtai/%s\" % x for x in files]\n",
        "\n",
        "for x, text in enumerate(transcribe(files)):\n",
        "  display(Audio(files[x]))\n",
        "  print(text)\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn3SlVE1LYvm"
      },
      "source": [
        "Overall, the results are solid. Each result sounds phonetically like the audio."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Whisper\n",
        "\n",
        "In September 2022, [OpenAI Whisper](https://github.com/openai/whisper) was released. This model brings a dramatic improvement in transcription quality. Whisper support was added to Hugging Face Transformers in v4.23.0. Let's give it a try."
      ],
      "metadata": {
        "id": "bDxW-tsCELob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe files\n",
        "transcribe = Transcription(\"openai/whisper-base\")\n",
        "for text in transcribe(files):\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "-KgYwAQzFVll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results were transcribed with near perfect accuracy, amazing!\n",
        "\n",
        "This can also be run as a txtai application or API instance. Let's try a full indexing workflow with a txtai application."
      ],
      "metadata": {
        "id": "fgMr3B2_IoeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile workflow.yml\n",
        "writable: true\n",
        "\n",
        "embeddings:\n",
        "  path: sentence-transformers/nli-mpnet-base-v2\n",
        "  content: true\n",
        "\n",
        "transcription:\n",
        "  path: openai/whisper-base\n",
        "\n",
        "workflow:\n",
        "  index:\n",
        "    tasks:\n",
        "      - transcription\n",
        "      - index"
      ],
      "metadata": {
        "id": "k3Rb2OU9Mq3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from txtai.app import Application\n",
        "\n",
        "app = Application(\"workflow.yml\")\n",
        "\n",
        "list(app.workflow(\"index\", files))\n",
        "app.search(\"feel good story\", 1)"
      ],
      "metadata": {
        "id": "cj49U095IxTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This workflow transcribed the input files, loaded the transcriptions into an embeddings index and finally ran a search. Last thing we'll do is run the workflow as an API instance."
      ],
      "metadata": {
        "id": "k6ptCiEyR8p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CONFIG=workflow.yml uvicorn \"txtai.api:app\" &> api.log &\n",
        "!sleep 30\n",
        "\n",
        "# Run indexing workflow\n",
        "!curl -s -o /dev/null \\\n",
        "  -X POST \"http://localhost:8000/workflow\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"name\":\"index\", \"elements\":[\"txtai/Beijing_mobilises.wav\", \"txtai/Canadas_last_fully.wav\", \"txtai/Maine_man_wins_1_mil.wav\", \"txtai/Make_huge_profits.wav\", \"txtai/The_National_Park.wav\", \"txtai/US_tops_5_million.wav\"]}'\n",
        "\n",
        "# Test API search\n",
        "!curl \"http://localhost:8000/search?query=feel+good+story&limit=1\""
      ],
      "metadata": {
        "id": "XDpqwf8PNHNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, the same results as in Python and with an application."
      ],
      "metadata": {
        "id": "jfdVtwbBIN4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapping up\n",
        "\n",
        "There is a lot of development in the audio transcription space. In only a couple of lines of code, high-quality transcription models are now readily available!"
      ],
      "metadata": {
        "id": "VCU8zGGDXQ0Y"
      }
    }
  ]
}